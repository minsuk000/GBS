% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/GBS.R, R/GBS_Logit.R
\name{GBS}
\alias{GBS}
\title{Generative Bootstrap Sampler}
\usage{

GBS(X, y, model=c('linear','logistic','LAD'), S=100, M=100, V=20, lr_U=0.001,lr_L = 0.0001,

lr_power = 0.3, num_it = 5000, hidden_size = 300, L = 3, gpu_ind = 0)

}
\arguments{
\item{X}{The predictor}
\item{y}{The response}
\item{model}{The model of interest. Linear model: "linear"; logistic regression models: "logistic"; least absolute deviation regression: "LAD".}

\item{S}{The number of subgroups in the subgroup bootstrap}

\item{M}{The number of Monte Carlo samples used in each iteration}

\item{V}{Update size of weights for each iteration}

\item{lr_U}{Lower bound of learning rate}

\item{lr_L}{Upper bound of learning rate}

\item{lr_power}{decay power rate}

\item{num_it}{The number of SGD iterations}

\item{hidden_size}{The number of nodes for each hidden layer}

\item{L}{The number of hidden layers}

\item{gpu_ind}{The index of GPU used in the computation under multiple GPUs}
}
\value{
  \item{time_tr}{computation time for training the gerenator}
  \item{S}{The subgroup size}
  \item{V}{The number of weights to be updatedd at each iteration}
  \item{hidden_size}{ The number of nodes for each hidden layer}
  \item{L}{The number of hidden layers}
}
\description{
Train the generator of GBS. You need to install python (>=3.7) and pytorch in advance. For the details of installation, visit https://pytorch.org/get-started/locally/. This R package is a wrapepr of python (using pytorch) programs via "reticulate" R package.
}
\details{
}
\examples{
#### linear regression example
library(reticulate)
set.seed(82941)
n=500
p=50
bt0 = seq(-1,1,length.out = p)
X = matrix(rnorm(n*p),n,p)
mu0 = crossprod(t(X),bt0)
y = mu0 + rnorm(n)
fit = lm(y~0+X)
theta = fit$coefficients
##############################
#### Training steps
dat = data.frame(X,y)
model = "linear"
#fit_Linear = GBS(X,y, model=model)
#### If you want to directly get bootstrap sample
#boots_samples = GBS_Sampling()
#### Post processing (CI constgruction and bias-correction)
#res = post_process(theta)
#### 0.95 CI construction
#par(mfrow=c(2,2), mai = c(0.4,0.7,0.3,0.1))
#title = c("CI1","CI2","CI3","CI_perc")
#for(i in 1:4){
#  ci0 = res[[i]]
#  plot(bt0,pch=19,cex=0.7,ylim=c(-1.3,1.3),
#       xlab="Index", ylab="Coefficient", main=title[i])
#  for(j in 1:p){
#    lines(c(j,j), c(ci0[j,1], ci0[j,2]),col="red",lwd=1.3)
#  }
#  points(ci0[,1], col="red", pch=16, cex=0.8)
#  points(ci0[,2], col="red", pch=16, cex=0.8)
#}
#### bias-correction
#par(mfrow=c(2,2), mai = c(0.4,0.7,0.3,0.1))
#title = c("theta_bc1","theta_bc2","theta_bc3")
#plot(bt0,bt0,pch=19,cex=1,ylim=c(-1.3,1.3), type="n",
#     xlab="Index", ylab="Coefficient", main="theta_MLE")
#points(bt0, bt0, col="blue", pch=16, cex=0.7)
#points(bt0, theta, col="red", pch=4, cex=1)
#for(i in 1:3){
#  ci0 = res[[8+i]]
#  plot(bt0,bt0,pch=19,cex=1,ylim=c(-1.3,1.3), type="n",
#       xlab="Index", ylab="Coefficient", main=title[i])
#  points(bt0, bt0, col="blue", pch=16, cex=0.7)
#  points(bt0, ci0, col="red", pch=4, cex=1)
#}



}
\seealso{
diagnosis
}
\author{
Minsuk Shin and Jun Liu
}
