% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/GBS_custom.R
\name{GBS_custom}
\alias{GBS_custom}
\title{Generative Bootstrap Sampler for Customized Loss}
\usage{
GBS_custom(dat, file_loss, S=100, M=100, V=10, lr_U=0.001,lr_L = 0.0001,

lr_power = 0.3, num_it = 5000, hidden_size = 300,L = 3, gpu_ind = 0)

}
\arguments{
\item{dat}{The data set in a form of data frame}

\item{file_loss}{The python code for the loss function in python of interest}

\item{S}{The number of subgroups in the subgroup bootstrap}

\item{M}{The number of Monte Carlo samples used in each iteration}

\item{V}{Update size of weights for each iteration}

\item{lr_U}{Lower bound of learning rate}

\item{lr_L}{Upper bound of learning rate}

\item{lr_power}{decay power rate}

\item{num_it}{The number of SGD iterations}

\item{hidden_size}{The number of nodes for each hidden layer}

\item{L}{The number of hidden layers}

\item{gpu_ind}{The index of GPU used in the computation under multiple GPUs}
}
\value{
  \item{time_tr}{computation time for training the gerenator}
  \item{S}{The subgroup size}
  \item{V}{The number of weights to be updatedd at each iteration}
  \item{hidden_size}{ The number of nodes for each hidden layer}
  \item{L}{The number of hidden layers}
}
\description{
Train the generator of GBS for a given customized loss function.
}
\details{
The class of "dat" should be a data frame. The object "file_loss" should be a name of a python code for the customized loss function; some illustrative examples are provided in the example section below.
}
\examples{
#### linear regression example
set.seed(82941)
n=500
p=50
bt0 = seq(-1,1,length.out = p)
X = matrix(rnorm(n*p),n,p)
mu0 = X%*%bt0
y = mu0 + rnorm(n)
fit = lm(y~0+X)
theta = fit$coefficients
##############################################################
#### "loss_Linear.py": python code for linear regression loss####################
#import os
#import torch
#import numpy as np
#import sys
#####Device configuration
#gpu_ind = int(r.gpu_ind)
#if torch.cuda.is_available():
#  device = torch.device('cuda', gpu_ind)
#else:
#  device = torch.device('cpu')
#"r.cpu_ind" in dir(os)
#try:
#   r.cpu_ind
#except:
#    print("No cpu_ind")
#else:
#    cpu_ind = int(r.cpu_ind)
#    if cpu_ind == 1:
#      device = torch.device('cpu')
#n = int(r.n)
#p = int(r.p)
#dat = r.dat
#dat = dat.values
#dat = torch.from_numpy(dat)
#dat = dat.to(device, dtype = torch.float)
#def D(dat, Theta):
#    c = torch.matmul(dat[:,0:p],Theta.t())
#    out =  -0.5*(dat[:,p] - c)**2
#    return out
#########################################################
#### Training steps
dat = data.frame(X,y)
#fit_Linear0 = GBS_custom(dat, file_loss="loss_Linear.py")
#### Post processing (CI constgruction and bias-correction)
#res = post_process(fit_Linear0, theta)

#########################################################
#### logistic regression example
set.seed(123948)
n=1000
p=20
bt0 = seq(-3,3,length.out = p)
X = matrix(rnorm(n*p),n,p)
mu0 = X%*%bt0
y = matrix(rbinom(n,1,1/(1+exp(-mu0))),n,1)
fit = glm(y~0+X, family=binomial())
theta = fit$coefficients

##############################################################
#### "loss_Logit.py": python code for logistic regression loss####################
#import os
#import torch
#import numpy as np
#import sys
#### Device configuration
#gpu_ind = int(r.gpu_ind)
#if torch.cuda.is_available():
#  device = torch.device('cuda', gpu_ind)
#else:
#  device = torch.device('cpu')
#"r.cpu_ind" in dir(os)
#try:
#   r.cpu_ind
#except:
#    print("No cpu_ind")
#else:
#    cpu_ind = int(r.cpu_ind)
#    if cpu_ind == 1:
#      device = torch.device('cpu')
#n = int(r.n)
#p = int(r.p)
#dat = r.dat
#dat = dat.values
#dat = torch.from_numpy(dat)
#dat = dat.to(device, dtype = torch.float)
#def D(dat, Theta):
#  c = torch.matmul(dat[:,0:p],Theta.t())
#  c = torch.clamp(c, -20, 20)
#  out =  -1.0*(1-dat[:,p].reshape(n,1))*c - torch.log(1.0+torch.exp(-c))
#  return out
#########################################################
#### Training steps
dat = data.frame(X,y)
#fit_Logit0 = GBS_custom(dat, file_loss="loss_Logit.py")
#### Post processing (CI constgruction and bias-correction)
#res = post_process(fit_Logit0, theta)

#########################################################
#### Diagnosis steps
#M=5
#S = 100
#alpha_cand = matrix(rexp(M*S),M,S)
#theta_alpha = matrix(0,M,p)
#for(m in 1:M){
#  alpha = matrix(alpha_cand[m,],1,S)
#  w = as.vector(A%*%t(alpha))
#  theta_alpha[m,] = glm( y ~ 0 + X, weights=w, family=binomial())$coefficients
#}
#diag = diagnosis(alpha_cand, theta_alpha)


}
\seealso{
`other_function`
}
\author{
Minsuk Shin and Jun Liu
}
